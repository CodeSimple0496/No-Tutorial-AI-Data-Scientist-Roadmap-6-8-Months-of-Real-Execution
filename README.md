# No-Tutorial-AI-Data-Scientist-Roadmap-6-8-Months-of-Real-Execution
A practical, execution-first roadmap designed to build an interview-ready AI &amp; Data Scientist profile through real datasets, disciplined GitHub commits, and end-to-end projects.

## Day 1 – Foundations & Environment Setup

- Set up browser-based Python environment using Google Colab
- Created and structured GitHub repository
- Practiced Python fundamentals
- Loaded and explored a CSV dataset using pandas
- Performed basic data understanding and data quality checks
- Wrote simple data-cleaning functions
- Documented learnings and concepts related to data and features


## Day 2 – Pandas & Data Exploration  

- Explored datasets using pandas for structured data analysis  
- Inspected dataset schema, data types, and dimensions  
- Performed filtering, sorting, and feature selection  
- Applied aggregation and groupby for business insights  
- Handled missing data and cleaned raw records  
- Conducted exploratory data analysis on employee attrition data  
- Created visualizations to highlight key attrition drivers  
- Documented findings and learning notes  
- Version-controlled all code and notebooks on GitHub

## Day 3 – Data Visualization and Exploratory Data Analysis 

- Implemented data visualization using Matplotlib and Seaborn  
- Created histograms, bar plots, box plots, and count plots  
- Analyzed feature distributions and attrition trends visually  
- Compared demographic and compensation factors against attrition  
- Performed correlation analysis and visualized heatmap  
- Summarized visual insights and key business patterns  
- Documented learnings and observations  
- Version-controlled all notebooks, plots, and notes on GitHub

## Day 4 – Feature Engineering & Data Preprocessing  

- Learned how to prepare raw data for machine learning models  
- Handled categorical features using Label Encoding and One-Hot Encoding  
- Applied feature scaling using Min-Max Scaling and Standardization  
- Created new engineered features (AgeGroup, Experience buckets, YearsInCompany)  
- Identified target variable (LeaveOrNot) and separated features and labels  
- Performed train-test split for model preparation  
- Built a mini preprocessing pipeline for cleaning, encoding, and scaling data  
- Documented learnings and preprocessing concepts in notes  
- Pushed notebooks, scripts, and notes to GitHub  


